{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f3fcb3-ae82-4a7f-ac7b-6a80d94658a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json, re, time\n",
    "\n",
    "def _vertex_available():\n",
    "    try:\n",
    "        import vertexai  # noqa\n",
    "        return os.environ.get(\"GOOGLE_CLOUD_PROJECT\") is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _make_vertex_call_llm():\n",
    "    import vertexai\n",
    "    from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "    from google.api_core.exceptions import NotFound, PermissionDenied, FailedPrecondition\n",
    "\n",
    "    PROJECT  = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
    "    MODEL = os.environ[\"GOOGLE_CLOUD_VERTEX_MODEL\"] = \"gemini-2.5-flash\"\n",
    "    REGION = os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"global\"\n",
    "\n",
    "\n",
    "    if not PROJECT:\n",
    "        raise EnvironmentError(\"GOOGLE_CLOUD_PROJECT not set\")\n",
    "\n",
    "    vertexai.init(project=PROJECT, location=REGION)\n",
    "\n",
    "    _cache = {}\n",
    "\n",
    "    def _safe_json(text: str) -> str:\n",
    "        try:\n",
    "            return json.dumps(json.loads(text))\n",
    "        except Exception:\n",
    "            t = text.strip()\n",
    "            if t.startswith(\"```\"):\n",
    "                t = t.strip(\"`\").split(\"\\n\", 1)[-1]\n",
    "                try:\n",
    "                    return json.dumps(json.loads(t))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return json.dumps({\"raw\": text})\n",
    "\n",
    "    def call_llm_vertex(\n",
    "        prompt: str,\n",
    "        system: str | None = None,\n",
    "        json_schema: dict | None = None,\n",
    "        model_name: str = MODEL,\n",
    "        temperature: float = 0.2,\n",
    "        max_output_tokens: int = 4096,\n",
    "        top_p: float = 0.95,\n",
    "        top_k: int = 40,\n",
    "    ) -> str:\n",
    "        key = (system or \"\")\n",
    "        if key not in _cache:\n",
    "            kwargs = {}\n",
    "            if system:\n",
    "                kwargs[\"system_instruction\"] = system\n",
    "            try:\n",
    "                model = GenerativeModel(model_name=model_name, **kwargs)\n",
    "                _ = model.generate_content(\"ping\", generation_config=GenerationConfig(max_output_tokens=1))\n",
    "                _cache[key] = model\n",
    "            except (NotFound, PermissionDenied, FailedPrecondition) as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Vertex model not accessible: region={REGION}, model={MODEL}. \"\n",
    "                    \"Enable Vertex AI API, grant Vertex/Generative AI roles, and ensure org policy allows this model/region.\"\n",
    "                ) from e\n",
    "\n",
    "        model = _cache[key]\n",
    "\n",
    "        if json_schema:\n",
    "            gen_cfg = GenerationConfig(\n",
    "                temperature=temperature,\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=json_schema,\n",
    "            )\n",
    "        else:\n",
    "            gen_cfg = GenerationConfig(\n",
    "                temperature=temperature,\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "            )\n",
    "\n",
    "        last_err = None\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                resp = model.generate_content(prompt, generation_config=gen_cfg)\n",
    "                text = getattr(resp, \"text\", None)\n",
    "                if text is None:\n",
    "                    parts = []\n",
    "                    for c in getattr(resp, \"candidates\", []) or []:\n",
    "                        for p in getattr(c, \"content\", []).parts:\n",
    "                            parts.append(getattr(p, \"text\", \"\") or str(p))\n",
    "                    text = \"\\n\".join([p for p in parts if p])\n",
    "                return _safe_json(text) if json_schema else text\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                time.sleep(0.5 * (2 ** attempt))\n",
    "        raise last_err\n",
    "\n",
    "    return call_llm_vertex\n",
    "\n",
    "# ---- choose active path ----\n",
    "USE_VERTEX = _vertex_available()\n",
    "try:\n",
    "    call_llm = _make_vertex_call_llm()\n",
    "    if not USE_VERTEX:\n",
    "        print(\"[INFO] Vertex AI not available or GOOGLE_CLOUD_PROJECT unset; using safe fallback.\")\n",
    "except Exception as _e:\n",
    "    print(f\"[WARN] Vertex AI unavailable ({type(_e).__name__}: {_e}). Using safe fallback.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287fc7b3-4649-45ed-886f-ec9cb43295b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Dict, Any, Callable\n",
    "\n",
    "INTENT_DEFINITIONS = [\n",
    "    {\n",
    "        \"intent\": \"Greeting\",\n",
    "        \"description\": \"A simple greeting or conversational opening. (e.g., 'Hello', 'Hi')\"\n",
    "    },\n",
    "    {\n",
    "        \"intent\": \"FAQ_Simple\",\n",
    "        \"description\": \"A simple question that can be answered with a knowledge base lookup. (e.g., 'What is the return policy?', 'When is my bill?')\"\n",
    "    },\n",
    "    {\n",
    "        \"intent\": \"Product_Inquiry\",\n",
    "        \"description\": \"A user is asking for details about a product or service. (e.g., 'Does this work with a Mac?', 'Is the Nespresso machine red?')\"\n",
    "    },\n",
    "    {\n",
    "        \"intent\": \"Billing_Question\",\n",
    "        \"description\": \"A user has a specific question about a charge or their bill. (e.g., 'Why was I charged $50?', 'What is this fee?')\"\n",
    "    },\n",
    "    {\n",
    "        \"intent\": \"Account_Lock\",\n",
    "        \"description\": \"A user is locked out of their account or needs a password reset.\"\n",
    "    },\n",
    "    {\n",
    "        \"intent\": \"Bug_Report_Complex\",\n",
    "        \"description\": \"A user is reporting a complex technical issue. (e.g., 'My site is down', 'I see error 404')\"\n",
    "    },\n",
    "    {\n",
    "        \"intent\": \"Cancel_Order\",\n",
    "        \"description\": \"A user explicitly wants to cancel a recent order. (e.g., 'I need to cancel order #12345')\"\n",
    "    },\n",
    "    {\n",
    "        \"intent\": \"Unknown\",\n",
    "        \"description\": \"The user's query is unclear, out of scope, or cannot be categorized.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "PROMPT_TEMPLATES = {\n",
    "    \"1.0\": {\n",
    "        \"system_persona\": \"You are a specialized intent classifier. Your single function is to output accurate JSON.\",\n",
    "        \"n_shot_examples\": [\n",
    "            {\"query\": \"My account is locked.\", \"intent\": \"Account_Lock\", \"confidence\": \"0.98\"},\n",
    "            {\"query\": \"When is my bill due?\", \"intent\": \"Billing_FAQ\", \"confidence\": \"0.95\"},\n",
    "            {\"query\": \"What is the store refund policy?\", \"intent\": \"FAQ_Simple\", \"confidence\": \"0.99\"}\n",
    "        ]\n",
    "    },\n",
    "    \"2.0\": {\n",
    "        \"system_persona\": \"You are a general classifier focused on maximizing coverage.\",\n",
    "        \"n_shot_examples\": [\n",
    "            {\"query\": \"Help me reset.\", \"intent\": \"Account_Lock\", \"confidence\": \"0.85\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_classification_schema() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    [BEHAVIOR] Defines the JSON Schema for the classification output.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"intent\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The primary intent of the user's query (e.g., FAQ_Simple, Bug_Report_Complex).\"\n",
    "            },\n",
    "            \"confidence\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"A confidence score between 0.0 and 1.0 that the classification is correct.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"intent\", \"confidence\"]\n",
    "    }\n",
    "\n",
    "def get_prompt_template(version: str) -> Dict[str, Any]:\n",
    "    \"\"\"[BEHAVIOR] Implements Prompt Versioning.\n",
    "    Retrieves a specific, versioned template definition.\n",
    "    \"\"\"\n",
    "    return PROMPT_TEMPLATES.get(version, PROMPT_TEMPLATES[\"1.0\"])\n",
    "\n",
    "\n",
    "def classify_and_structure(user_query: str) -> Dict[str, Any]:\n",
    "    \"\"\"[BEHAVIOR] Implements Structured Prompting, Templating, and N-Shot Prompting.\n",
    "    Constructs a detailed prompt to force the LLM into a reliable classification behavior.\n",
    "    \"\"\"\n",
    "    # 1. Prompt Versioning: Select the latest stable version\n",
    "    template = get_prompt_template(\"1.0\")\n",
    "    \n",
    "    # Construct N-Shot examples string\n",
    "    n_shot_str = \"\\n\".join([\n",
    "        f\"INPUT: {ex['query']}\\nOUTPUT: {json.dumps({'intent': ex['intent'], 'confidence': ex['confidence']})}\"\n",
    "        for ex in template['n_shot_examples']\n",
    "    ])\n",
    "    \n",
    "    # 2. Structured Prompt with Prompt Templating\n",
    "    # This template is used to construct the final instruction to the LLM\n",
    "    valid_intents_str = \"\\n\".join([\n",
    "        f\"- {i['intent']}: {i['description']}\" for i in INTENT_DEFINITIONS\n",
    "    ])\n",
    "    \n",
    "    structured_prompt_template = (\n",
    "        f\"{template['system_persona']}\\n\\n\"\n",
    "        f\"You must analyze the user's INPUT and return ONLY a single, valid JSON object.\\n\"\n",
    "        f\"Your output JSON must contain 'intent' (string) and 'confidence' (float, 0.0-1.0).\\n\\n\"\n",
    "        f\"--- VALID INTENTS ---\\n\"\n",
    "        f\"You MUST choose one of the following intents:\\n\"\n",
    "        f\"{valid_intents_str}\\n\\n\"\n",
    "        f\"--- EXAMPLES ({len(template['n_shot_examples'])} Shot Prompting) ---\\n\"\n",
    "        f\"{n_shot_str}\\n\\n\"\n",
    "        f\"--- CLASSIFICATION TASK ---\\n\"\n",
    "        f\"INPUT: {user_query}\\n\"\n",
    "        f\"OUTPUT:\"\n",
    "    )\n",
    "\n",
    "    # 3. Call LLM with the structured prompt\n",
    "    raw_response = call_llm(\n",
    "        prompt=structured_prompt_template, \n",
    "        json_schema=get_classification_schema()\n",
    "    )    \n",
    "    \n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        # Assuming the LLM returns only the JSON object\n",
    "        data = json.loads(raw_response)\n",
    "        data[\"priority\"] = \"High\" if data.get(\"confidence\", 0.0) < 0.6 else \"Medium\"\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing LLM response: {e}\")\n",
    "        return {\"intent\": \"Error\", \"priority\": \"High\", \"confidence\": 0.0}\n",
    "    \n",
    "# print(classify_and_structure(\"can I return Nespresso\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63725488-db0e-40b5-8889-004144f7f0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /opt/conda/lib/python3.10/site-packages (1.3.4)\n",
      "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (5.1.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.15.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.22.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.20.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (30.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /opt/conda/lib/python3.10/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2025.8.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.56.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (6.31.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.38.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.10/site-packages (from triton==3.4.0->torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1191a914-4524-4cd8-ade8-36d1565d9809",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® [VectorDB Setup] Initializing ChromaDB and indexing data...\n",
      "‚úÖ [VectorDB Setup] Indexed 6 documents into 'company_knowledge_base'.\n"
     ]
    }
   ],
   "source": [
    "# retrieval.py\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "# --- GLOBAL COMPONENTS (RAG Infrastructure) ---\n",
    "# 1. Embedding Model: Used to convert query/documents into vectors\n",
    "EMBEDDING_MODEL = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "# Define the embedding function for Chroma\n",
    "# def embed_function(texts: List[str]) -> List[List[float]]:\n",
    "#     return EMBEDDING_MODEL.encode(texts).tolist()\n",
    "\n",
    "# 2. Vector DB Client and Collection (The \"real\" Vector DB)\n",
    "CLIENT = chromadb.Client()\n",
    "COLLECTION_NAME = \"company_knowledge_base\"\n",
    "EMBEDDING_FUNCTION = SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# --- COMPANY KNOWLEDGE DATA ---\n",
    "COMPANY_DOCUMENTS = [\n",
    "    {\"id\": \"doc_001\", \"content\": \"The standard return policy is 30 days for electronics (like Nespresso machines), provided the original packaging is intact.\", \"source\": \"Policy\"},\n",
    "    {\"id\": \"doc_002\", \"content\": \"Error code 404 is a client-side network error, typically resolved by clearing the browser cache or checking the URL.\", \"source\": \"TechDoc\"},\n",
    "    {\"id\": \"doc_003\", \"content\": \"Annual service fees are due on the 1st of January and are non-refundable after 7 days. The monthly subscription fee is $50.\", \"source\": \"Billing\"},\n",
    "    {\"id\": \"doc_004\", \"content\": \"Our new security update requires all users to enable two-factor authentication by the end of the quarter.\", \"source\": \"Policy\"},\n",
    "    {\"id\": \"doc_005\", \"content\": \"To cancel an order, you must log in to your portal and hit 'Cancel' within 2 hours of placing the order. After 2 hours, orders cannot be canceled.\", \"source\": \"Policy\"},\n",
    "    {\"id\": \"doc_006\", \"content\": \"The Nespresso Vertuo machine is available in Red, Black, and Chrome.\", \"source\": \"Product\"}\n",
    "]\n",
    "\n",
    "def setup_vector_db():\n",
    "    \"\"\"Initializes the Vector DB and indexes the documents.\"\"\"\n",
    "    print(\"‚ú® [VectorDB Setup] Initializing ChromaDB and indexing data...\")\n",
    "    # Create the collection, specifying the embedding function\n",
    "    collection = CLIENT.get_or_create_collection(\n",
    "        name=COLLECTION_NAME, \n",
    "        embedding_function=EMBEDDING_FUNCTION\n",
    "    )\n",
    "\n",
    "    documents = [d['content'] for d in COMPANY_DOCUMENTS]\n",
    "    metadatas = [{\"source\": d['source']} for d in COMPANY_DOCUMENTS]\n",
    "    ids = [d['id'] for d in COMPANY_DOCUMENTS]\n",
    "\n",
    "    # Index the documents\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    print(f\"‚úÖ [VectorDB Setup] Indexed {len(documents)} documents into '{COLLECTION_NAME}'.\")\n",
    "    return collection\n",
    "\n",
    "# Initialize the collection (run this once when the application starts)\n",
    "KNOWLEDGE_COLLECTION = setup_vector_db()\n",
    "\n",
    "\n",
    "def perform_rag_search(query: str, required_k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    [RETRIEVAL] Implements the RAG pattern with Data Source Router and Ranking.\n",
    "    Uses the ChromaDB client to perform real vector search.\n",
    "    \"\"\"\n",
    "    if KNOWLEDGE_COLLECTION is None:\n",
    "        print(\"üö® [Retrieval] Vector DB not initialized.\")\n",
    "        return \"\"\n",
    "        \n",
    "    print(f\"üìö [Retrieval] Searching Vector DB for relevant context (k={required_k})...\")\n",
    "    \n",
    "    # 1. Retrieval Router (Metadata Filter)\n",
    "    # This acts as a router/filter to scope the vector search\n",
    "    metadata_filter = {}\n",
    "    if any(keyword in query.lower() for keyword in [\"policy\", \"refund\", \"return\", \"cancel\"]):\n",
    "        metadata_filter = {\"source\": \"Policy\"}\n",
    "        print(\"  -> Retrieval Router applied filter: 'Policy'\")\n",
    "    elif any(keyword in query.lower() for keyword in [\"error\", \"fix\", \"cache\", \"404\"]):\n",
    "        metadata_filter = {\"source\": \"TechDoc\"}\n",
    "        print(\"  -> Retrieval Router applied filter: 'TechDoc'\")\n",
    "    elif any(keyword in query.lower() for keyword in [\"bill\", \"charge\", \"fee\", \"$50\"]):\n",
    "        metadata_filter = {\"source\": \"Billing\"}\n",
    "        print(\"  -> Retrieval Router applied filter: 'Billing'\")\n",
    "    elif any(keyword in query.lower() for keyword in [\"color\", \"Nespresso\", \"mac\"]):\n",
    "        metadata_filter = {\"source\": \"Product\"}\n",
    "        print(\"  -> Retrieval Router applied filter: 'Product'\")\n",
    "    \n",
    "   # 2. Vector Search (Retrieval) \n",
    "    if metadata_filter:\n",
    "        print(f\"  -> Retrieval Router applied filter: {metadata_filter}\")\n",
    "        results = KNOWLEDGE_COLLECTION.query(\n",
    "            query_texts=[query],\n",
    "            n_results=required_k,\n",
    "            where=metadata_filter # Apply the router filter\n",
    "        )\n",
    "    else:\n",
    "        print(\"  -> Retrieval Router: No filter applied, searching all documents.\")\n",
    "        results = KNOWLEDGE_COLLECTION.query(\n",
    "            query_texts=[query],\n",
    "            n_results=required_k\n",
    "            # No 'where' parameter is passed\n",
    "        )\n",
    "\n",
    "    if not results or not results.get('documents') or not results['documents'][0]:\n",
    "        print(\"‚ö†Ô∏è No context retrieved.\")\n",
    "        return \"\"\n",
    "\n",
    "    # 3. Ranking (Slicing)\n",
    "    # Chroma returns results sorted by distance. For this demo, \"ranking\"\n",
    "    # means we just take the top 'required_k' that Chroma already ranked.\n",
    "    # A true \"Re-ranking\" pattern would fetch more (e.g., k*2) and use a\n",
    "    # second model to re-order them for relevance.\n",
    "    \n",
    "    ranked_chunks = []\n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        chunk = {\n",
    "            \"text\": doc,\n",
    "            \"source\": results['metadatas'][0][i]['source'],\n",
    "            \"similarity_score\": 1.0 - results['distances'][0][i] \n",
    "        }\n",
    "        ranked_chunks.append(chunk)\n",
    "\n",
    "    final_chunks = ranked_chunks[:required_k]\n",
    "\n",
    "    # 4. Context Formatting (for the LLM)\n",
    "    context_list = []\n",
    "    for i, chunk in enumerate(final_chunks):\n",
    "        context_list.append(\n",
    "            f\"Source {i+1} [Type: {chunk['source']} | Score: {chunk['similarity_score']:.3f}]: {chunk['text']}\"\n",
    "        )\n",
    "\n",
    "    print(f\"  -> Ranking completed. Selected {len(final_chunks)} chunks for context.\")\n",
    "    return \"\\n---\\n\".join(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf30747-d9a2-48d5-a7e7-bb4be10124c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# governance.py\n",
    "import random\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "MIN_CONFIDENCE_THRESHOLD = 0.85 # Policy for classification confidence\n",
    "MIN_FAITHFULNESS_SCORE = 0.75   # Policy for LLM Judge output\n",
    "\n",
    "def get_judge_schema() -> Dict[str, Any]:\n",
    "    \"\"\"Defines the JSON Schema for the Judge's structured output.\"\"\"\n",
    "    return {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"evaluation\": {\"type\": \"string\", \"enum\": [\"PASS\", \"FAIL\"]},\n",
    "            \"reasoning\": {\"type\": \"string\"},\n",
    "            \"faithfulness_score\": {\"type\": \"number\"}\n",
    "        },\n",
    "        \"required\": [\"evaluation\", \"reasoning\", \"faithfulness_score\"]\n",
    "    }\n",
    "def check_guardrails(\n",
    "    text: str, \n",
    "    is_input: bool, \n",
    "    user_query: Optional[str] = None, \n",
    "    rag_context: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    [GOVERNANCE] Comprehensive Guardrail Check (Simple + LLM Judge).\n",
    "    Returns a dictionary: {\"status\": \"PASS\"/\"FAIL\", \"reason\": \"...\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. INPUT Guardrails (Fast Checks)\n",
    "    if is_input:\n",
    "        if \"admin access\" in text.lower():\n",
    "            return {\"status\": \"FAIL\", \"reason\": \"Input BLOCKED: Prompt injection attempt.\"}\n",
    "        if any(w in text.lower() for w in [\"kill\", \"bomb\", \"threat\"]):\n",
    "            return {\"status\": \"FAIL\", \"reason\": \"Input BLOCKED: Detected toxic language.\"}\n",
    "        print(\"‚úÖ [Guardrail] Input passed safety checks.\")\n",
    "        return {\"status\": \"PASS\"}\n",
    "\n",
    "    \n",
    "    # 2. OUTPUT Guardrails (Simple/Fast Checks)\n",
    "    if any(s in text for s in [\"123-456-7890\", \"confidential\"]):\n",
    "        return {\"status\": \"FAIL\", \"reason\": \"Output BLOCKED: Detected sensitive data (PII).\"}\n",
    "    if any(w in text.lower() for w in [\"hate\", \"illegal\"]):\n",
    "        return {\"status\": \"FAIL\", \"reason\": \"Output BLOCKED: Detected toxic language.\"}\n",
    "    print(\"‚úÖ [Guardrail] Output passed simple safety checks.\")\n",
    "    \n",
    "    # 3. LLM Judge Guardrail (Complex Check for RAG Quality)\n",
    "    # Only run the complex Judge if we have context (i.e., this was a RAG answer)\n",
    "    if rag_context and user_query:\n",
    "        print(\"‚öñÔ∏è [Judge] Starting LLM-as-a-Judge evaluation for faithfulness...\")\n",
    "        \n",
    "        judge_prompt = (\n",
    "            f\"You are a Quality Control Judge for a customer support bot. \"\n",
    "            f\"Your task is to evaluate a generated answer based on three criteria: \"\n",
    "            f\"**Faithfulness/Hallucination**, **Helpfulness**, and **Safety**.\\n\\n\"\n",
    "\n",
    "            f\"--- CONTEXT ---\\n{rag_context}\\n\\n\"\n",
    "            f\"--- USER QUERY ---\\n{user_query}\\n\\n\"\n",
    "            f\"--- ANSWER TO EVALUATE ---\\n{text}\\n\\n\"\n",
    "\n",
    "            f\"--- EVALUATION TASK ---\\n\"\n",
    "            f\"1. **Faithfulness/Hallucination**: Does the answer only contain facts that are explicitly supported by the CONTEXT? If yes, score 1.0. If it contains unsupported facts (Hallucination), score 0.0.\\n\"\n",
    "            f\"2. **Helpfulness**: Does the answer directly address the user query?\\n\"\n",
    "            f\"3. **Verdict**: If the faithfulness score is below {MIN_FAITHFULNESS_SCORE}, or if the answer is unhelpful, set 'evaluation' to 'FAIL'. Otherwise, 'PASS'.\\n\\n\"\n",
    "\n",
    "            f\"You must return ONLY a single, valid JSON object matching the required schema.\"\n",
    "        )\n",
    "        judge_model = \"gemini-2.5-pro\"\n",
    "        \n",
    "        try:\n",
    "            raw_response = call_llm(prompt=judge_prompt, model_name=judge_model, json_schema=get_judge_schema())\n",
    "            evaluation = json.loads(raw_response)\n",
    "            \n",
    "            # Policy Enforcement: Check faithfulness score\n",
    "            if evaluation.get(\"faithfulness_score\", 0.0) < MIN_FAITHFULNESS_SCORE:\n",
    "                return {\n",
    "                    \"status\": \"FAIL\", \n",
    "                    \"reason\": f\"Judge FAIL: Hallucination score ({evaluation['faithfulness_score']:.2f}) below policy.\",\n",
    "                    \"details\": evaluation\n",
    "                }\n",
    "            \n",
    "            print(f\"‚úÖ [Judge] Hallucination Check PASSED (Faithfulness: {evaluation.get('faithfulness_score'):.2f}).\")\n",
    "            return {\"status\": \"PASS\", \"details\": evaluation}\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If the Judge itself fails to run or parse, fail the process for safety\n",
    "            return {\"status\": \"FAIL\", \"reason\": f\"Judge Execution Error: {e}\"}\n",
    "\n",
    "    # 4. Final PASS (For non-RAG output that passed simple checks)\n",
    "    return {\"status\": \"PASS\"}\n",
    "\n",
    "def check_for_escalation(intent: str, confidence: float) -> bool:\n",
    "    \"\"\"\n",
    "    [GOVERNANCE] Implements the Confidence & Escalation Pattern.\n",
    "    Returns True if the request MUST be escalated.\n",
    "    \"\"\"\n",
    "    if intent in [\"Error\", \"Unknown\"]:\n",
    "        print(f\"üö® [Governance] Intent '{intent}' requires immediate escalation.\")\n",
    "        return True\n",
    "        \n",
    "    if confidence < MIN_CONFIDENCE_THRESHOLD:\n",
    "        print(f\"üö® [Governance] Confidence ({confidence:.2f}) is below policy threshold ({MIN_CONFIDENCE_THRESHOLD}). ESCALATION REQUIRED.\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"‚úÖ [Governance] Confidence ({confidence:.2f}) is above policy threshold.\")\n",
    "    return False\n",
    "\n",
    "def escalate_to_hitl(user_query: str, intent: str, confidence: float) -> Dict[str, Any]:\n",
    "    \"\"\"[GOVERNANCE] Handles the final escalation action.\"\"\"\n",
    "    print(\"‚û°Ô∏è [Governance] Routing request to Human-in-the-Loop (HITL) queue.\")\n",
    "    # In a real system, this would write to a database or a pub/sub topic.\n",
    "    return {\n",
    "        \"status\": \"ESCALATED\", \n",
    "        \"reason\": f\"Model uncertainty for intent '{intent}' (Confidence: {confidence:.2f}).\",\n",
    "        \"payload\": user_query\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22194df-4ec4-40b1-9a22-5409152d36c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# orchestration.py\n",
    "from typing import Callable, Dict, Any\n",
    "# We import the placeholder\n",
    "\n",
    "# --- MOCK TOOL CALL HANDLER ---\n",
    "def _handle_tool_call(tool_name: str, arguments: Dict[str, Any]) -> str:\n",
    "    \"\"\"Simulates executing an external function call.\"\"\"\n",
    "    print(f\"üõ†Ô∏è [Tool Handler] Executing tool: {tool_name} with args: {arguments}\")\n",
    "    if tool_name == \"get_billing_details\":\n",
    "        return f\"Tool Response: The user was charged $50 for 'Monthly Subscription Fee' on {arguments.get('date', '2025-11-15')}.\"\n",
    "    elif tool_name == \"trigger_cancellation_workflow\":\n",
    "        return f\"Tool Response: Workflow initiated for order {arguments.get('order_id', 'unknown')}.\"\n",
    "    return f\"Tool {tool_name} executed successfully.\"\n",
    "\n",
    "\n",
    "# --- ROUTER CORE LOGIC ---\n",
    "\n",
    "def select_model_and_context(\n",
    "    intent: str, \n",
    "    query: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    [ORCHESTRATION] Router Logic: Determines the best resource (Model, Tool, RAG)\n",
    "    based on the classified intent.\n",
    "    \"\"\"\n",
    "    if intent == \"Greeting\":\n",
    "        return {\"resource_type\": \"model\", \"model_name\": \"gemini-2.5-flash\", \"use_rag\": False, \"tool_call\": None}\n",
    "        \n",
    "    elif intent == \"FAQ_Simple\" or intent == \"Product_Inquiry\":\n",
    "        # Simple RAG with a fast model\n",
    "        return {\"resource_type\": \"model\", \"model_name\": \"gemini-2.5-flash\", \"use_rag\": True, \"tool_call\": None}\n",
    "    \n",
    "    elif intent == \"Bug_Report_Complex\":\n",
    "        # Complex RAG with a powerful model\n",
    "        return {\"resource_type\": \"model\", \"model_name\": \"gemini-2.5-pro\", \"use_rag\": True, \"tool_call\": None}\n",
    "    \n",
    "    elif intent == \"Billing_Question\":\n",
    "        # Example: This intent triggers a tool call first\n",
    "        return {\n",
    "            \"resource_type\": \"tool\",\n",
    "            \"model_name\": \"gemini-2.5-flash\", # Model to use *after* tool\n",
    "            \"use_rag\": False, # Tool provides context\n",
    "            \"tool_call\": {\n",
    "                \"name\": \"get_billing_details\", \n",
    "                \"arguments\": {\"date\": \"2025-11-15\"} # Mocked args\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    elif intent == \"Cancel_Order\":\n",
    "        # Example: This intent triggers a tool call\n",
    "        return {\n",
    "            \"resource_type\": \"tool\",\n",
    "            \"model_name\": \"gemini-2.5-flash\",\n",
    "            \"use_rag\": False, \n",
    "            \"tool_call\": {\n",
    "                \"name\": \"trigger_cancellation_workflow\", \n",
    "                \"arguments\": {\"order_id\": \"12345\"} # Mocked args\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Default (e.g., Account_Lock, Unknown)\n",
    "    return {\"resource_type\": \"model\", \"model_name\": \"gemini-2.5-pro\", \"use_rag\": True, \"tool_call\": None}\n",
    "\n",
    "\n",
    "def execute_generation(\n",
    "    route: Dict[str, Any],\n",
    "    query: str,\n",
    "    rag_context: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    [ORCHESTRATION] Final execution function\n",
    "    \"\"\"\n",
    "    resource_type = route['resource_type']\n",
    "    model_name = route['model_name']\n",
    "    \n",
    "    # 1. Handle Tool-based routes\n",
    "    if resource_type == \"tool\":\n",
    "        tool_call = route['tool_call']\n",
    "        print(f\"‚öôÔ∏è [Router] Delegating task to Tool: {tool_call['name']}\")\n",
    "        \n",
    "        # Execute the tool\n",
    "        tool_response = _handle_tool_call(tool_call['name'], tool_call['arguments'])\n",
    "        \n",
    "        # Now, use the model to *summarize* the tool's output\n",
    "        final_prompt = (\n",
    "            f\"You are a helpful assistant. A user asked: '{query}'.\\n\"\n",
    "            f\"An internal tool was run and provided this data: '{tool_response}'.\\n\"\n",
    "            f\"Answer the user's question based *only* on the tool's response.\"\n",
    "        )\n",
    "        # Note: We are now calling the LLM as a *model*\n",
    "        \n",
    "    # 2. Handle Model-based routes\n",
    "    else:\n",
    "        print(f\"‚öôÔ∏è [Router] Delegating task to LLM: {model_name}\")\n",
    "        \n",
    "        # Construct the final prompt, conditionally adding RAG context\n",
    "        final_prompt = f\"USER QUESTION: {query}\"\n",
    "        if route['use_rag'] and rag_context:\n",
    "            final_prompt = (\n",
    "                f\"Use the following context to answer the user's question.\\n\"\n",
    "                f\"CONTEXT:\\n{rag_context}\\n\\n\"\n",
    "                f\"--- \\n\\n\"\n",
    "                f\"USER QUESTION: {query}\"\n",
    "            )\n",
    "        elif route['use_rag'] and not rag_context:\n",
    "             final_prompt = f\"USER QUESTION: {query}\\n\\n(Note: No specific context was found, answer generally.)\"\n",
    "    \n",
    "    # 3. Call the LLM with the final composed prompt\n",
    "    return call_llm(\n",
    "        prompt=final_prompt,\n",
    "        model_name=model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83a56e85-d2e5-45c7-8e77-42b5c1eb99a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main.py\n",
    "# import governance\n",
    "# import behavior\n",
    "# import retrieval\n",
    "# import orchestration\n",
    "from typing import Dict, Any\n",
    "\n",
    "def process_customer_query(user_query: str):\n",
    "    \"\"\"\n",
    "    Composes all design patterns into the Customer Support Query workflow.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"| NEW QUERY: {user_query}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. GOVERNANCE: Input Guardrail\n",
    "    if not check_guardrails(user_query, is_input=True):\n",
    "        return {\"status\": \"REJECTED\", \"reason\": \"Input guardrail violation.\"}\n",
    "\n",
    "    # 2. BEHAVIOR: Classification\n",
    "    # The behavior module uses the LLM to get structured intent\n",
    "    classification_data = classify_and_structure(user_query)\n",
    "    intent = classification_data.get(\"intent\")\n",
    "    confidence = classification_data.get(\"confidence\")\n",
    "    \n",
    "    print(f\"‚ÑπÔ∏è [Classifier] Intent={intent}, Confidence={confidence:.2f}\")\n",
    "\n",
    "    # 3. GOVERNANCE: Confidence Check (Escalation)\n",
    "    if check_for_escalation(intent, confidence):\n",
    "        return escalate_to_hitl(user_query, intent, confidence)\n",
    "\n",
    "    # 4. ORCHESTRATION: Routing\n",
    "    # Select the right model, RAG strategy, or tool based on intent\n",
    "    route = select_model_and_context(intent, user_query)\n",
    "    \n",
    "    # 5. RETRIEVAL: Conditional RAG\n",
    "    # Only perform RAG if the orchestrator's route says to\n",
    "    rag_context = \"\"\n",
    "    if route.get(\"use_rag\", False):\n",
    "        rag_context = perform_rag_search(user_query, required_k=3)\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è [Orchestrator] Skipping RAG for this route.\")\n",
    "\n",
    "    # 6. ORCHESTRATION: Execution\n",
    "    # Pass the route, query, and context (if any) to the executor\n",
    "    final_answer = execute_generation(\n",
    "        route=route,\n",
    "        query=user_query,\n",
    "        rag_context=rag_context\n",
    "    )\n",
    "        \n",
    "    # 7. GOVERNANCE: Output Guardrail\n",
    "    output_check = check_guardrails(\n",
    "        text=final_answer, \n",
    "        is_input=False,\n",
    "        user_query=user_query,\n",
    "        rag_context=rag_context\n",
    "    )\n",
    "        \n",
    "    if output_check[\"status\"] == \"FAIL\":\n",
    "        # Escalation is triggered here if the LLM Judge failed the answer quality\n",
    "        return escalate_to_hitl(\n",
    "            user_query, \n",
    "            intent=\"GuardrailFailed\", \n",
    "            confidence=0.0 # Force low confidence for escalation\n",
    "        )\n",
    "\n",
    "    print(\"\\n‚úÖ Process Complete: Answer delivered safely.\")\n",
    "    return {\"status\": \"SUCCESS\", \"answer\": final_answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78481fe-506b-4e69-946e-749b413b5f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "| NEW QUERY: can I return Nespresso\n",
      "==================================================\n",
      "‚úÖ [Guardrail] Input passed safety checks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è [Classifier] Intent=FAQ_Simple, Confidence=0.95\n",
      "‚úÖ [Governance] Confidence (0.95) is above policy threshold.\n",
      "üìö [Retrieval] Searching Vector DB for relevant context (k=3)...\n",
      "  -> Retrieval Router applied filter: 'Policy'\n",
      "  -> Retrieval Router applied filter: {'source': 'Policy'}\n",
      "  -> Ranking completed. Selected 3 chunks for context.\n",
      "‚öôÔ∏è [Router] Delegating task to LLM: gemini-2.5-flash\n",
      "‚úÖ [Guardrail] Output passed simple safety checks.\n",
      "‚öñÔ∏è [Judge] Starting LLM-as-a-Judge evaluation for faithfulness...\n",
      "‚úÖ [Judge] Hallucination Check PASSED (Faithfulness: 1.00).\n",
      "\n",
      "‚úÖ Process Complete: Answer delivered safely.\n",
      "{'status': 'SUCCESS', 'answer': 'Yes, you can return a Nespresso machine within 30 days, provided the original packaging is intact.'}\n",
      "\n",
      "==================================================\n",
      "| NEW QUERY: My site is down and I see a 404 error\n",
      "==================================================\n",
      "‚úÖ [Guardrail] Input passed safety checks.\n",
      "‚ÑπÔ∏è [Classifier] Intent=Bug_Report_Complex, Confidence=0.99\n",
      "‚úÖ [Governance] Confidence (0.99) is above policy threshold.\n",
      "üìö [Retrieval] Searching Vector DB for relevant context (k=3)...\n",
      "  -> Retrieval Router applied filter: 'TechDoc'\n",
      "  -> Retrieval Router applied filter: {'source': 'TechDoc'}\n",
      "  -> Ranking completed. Selected 1 chunks for context.\n",
      "‚öôÔ∏è [Router] Delegating task to LLM: gemini-2.5-pro\n",
      "‚úÖ [Guardrail] Output passed simple safety checks.\n",
      "‚öñÔ∏è [Judge] Starting LLM-as-a-Judge evaluation for faithfulness...\n",
      "‚úÖ [Judge] Hallucination Check PASSED (Faithfulness: 1.00).\n",
      "\n",
      "‚úÖ Process Complete: Answer delivered safely.\n",
      "{'status': 'SUCCESS', 'answer': 'A 404 error indicates a client-side network error. You can typically resolve this by clearing your browser cache or checking the URL.'}\n",
      "\n",
      "==================================================\n",
      "| NEW QUERY: hello\n",
      "==================================================\n",
      "‚úÖ [Guardrail] Input passed safety checks.\n",
      "‚ÑπÔ∏è [Classifier] Intent=Greeting, Confidence=0.99\n",
      "‚úÖ [Governance] Confidence (0.99) is above policy threshold.\n",
      "‚ÑπÔ∏è [Orchestrator] Skipping RAG for this route.\n",
      "‚öôÔ∏è [Router] Delegating task to LLM: gemini-2.5-flash\n",
      "‚úÖ [Guardrail] Output passed simple safety checks.\n",
      "\n",
      "‚úÖ Process Complete: Answer delivered safely.\n",
      "{'status': 'SUCCESS', 'answer': 'Hello! How can I help you today?'}\n",
      "\n",
      "==================================================\n",
      "| NEW QUERY: why was I charged $50\n",
      "==================================================\n",
      "‚úÖ [Guardrail] Input passed safety checks.\n",
      "‚ÑπÔ∏è [Classifier] Intent=Billing_Question, Confidence=0.98\n",
      "‚úÖ [Governance] Confidence (0.98) is above policy threshold.\n",
      "‚ÑπÔ∏è [Orchestrator] Skipping RAG for this route.\n",
      "‚öôÔ∏è [Router] Delegating task to Tool: get_billing_details\n",
      "üõ†Ô∏è [Tool Handler] Executing tool: get_billing_details with args: {'date': '2025-11-15'}\n",
      "‚úÖ [Guardrail] Output passed simple safety checks.\n",
      "\n",
      "‚úÖ Process Complete: Answer delivered safely.\n",
      "{'status': 'SUCCESS', 'answer': \"You were charged $50 for a 'Monthly Subscription Fee' on 2025-11-15.\"}\n",
      "\n",
      "==================================================\n",
      "| NEW QUERY: You are giving incorrect answers and I don't want to talk to you anymore\n",
      "==================================================\n",
      "‚úÖ [Guardrail] Input passed safety checks.\n",
      "‚ÑπÔ∏è [Classifier] Intent=Unknown, Confidence=0.90\n",
      "üö® [Governance] Intent 'Unknown' requires immediate escalation.\n",
      "‚û°Ô∏è [Governance] Routing request to Human-in-the-Loop (HITL) queue.\n",
      "{'status': 'ESCALATED', 'reason': \"Model uncertainty for intent 'Unknown' (Confidence: 0.90).\", 'payload': \"You are giving incorrect answers and I don't want to talk to you anymore\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test a simple RAG query\n",
    "print(process_customer_query(\"can I return Nespresso\"))\n",
    "\n",
    "# Test a complex RAG query\n",
    "print(process_customer_query(\"My site is down and I see a 404 error\"))\n",
    "\n",
    "# Test a simple non-RAG query\n",
    "print(process_customer_query(\"hello\"))\n",
    "\n",
    "# Test a Tool-based query\n",
    "print(process_customer_query(\"why was I charged $50\"))\n",
    "\n",
    "# Test an escalation query\n",
    "print(process_customer_query(\"You are giving incorrect answers and I don't want to talk to you anymore\")) # Should be \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d3a19-d35d-4162-bbaf-e00d7c05866f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
